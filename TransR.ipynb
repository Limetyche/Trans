{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-03-16T09:22:49.298009Z",
     "end_time": "2024-03-16T09:22:49.302503Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "class TrainSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super(TrainSet, self).__init__()\n",
    "        # self.raw_data, self.entity_dic, self.relation_dic = self.load_texd()\n",
    "        self.raw_data, self.entity_to_index, self.relation_to_index = self.load_text()\n",
    "        self.entity_num, self.relation_num = len(self.entity_to_index), len(self.relation_to_index)\n",
    "        self.triple_num = self.raw_data.shape[0]\n",
    "        print(f'Train set: {self.entity_num} entities, {self.relation_num} relations, {self.triple_num} triplets.')\n",
    "        self.pos_data = self.convert_word_to_index(self.raw_data)\n",
    "        self.related_dic = self.get_related_entity()\n",
    "        # print(self.related_dic[0], self.related_dic[479])\n",
    "        self.neg_data = self.generate_neg()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.triple_num\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return [self.pos_data[item], self.neg_data[item]]\n",
    "\n",
    "    def load_text(self):\n",
    "        raw_data = pd.read_csv('/kg/transe/fb15k/freebase_mtr100_mte100-train.txt', sep='\\t', header=None,\n",
    "                               names=['head', 'relation', 'tail'],\n",
    "                               keep_default_na=False, encoding='utf-8')\n",
    "        raw_data = raw_data.applymap(lambda x: x.strip())\n",
    "        head_count = Counter(raw_data['head'])\n",
    "        tail_count = Counter(raw_data['tail'])\n",
    "        relation_count = Counter(raw_data['relation'])\n",
    "        entity_list = list((head_count + tail_count).keys())\n",
    "        relation_list = list(relation_count.keys())\n",
    "        entity_dic = dict([(word, idx) for idx, word in enumerate(entity_list)])\n",
    "        relation_dic = dict([(word, idx) for idx, word in enumerate(relation_list)])\n",
    "        return raw_data.values, entity_dic, relation_dic\n",
    "\n",
    "    def convert_word_to_index(self, data):\n",
    "        index_list = np.array([\n",
    "            [self.entity_to_index[triple[0]], self.relation_to_index[triple[1]], self.entity_to_index[triple[2]]] for\n",
    "            triple in data])\n",
    "        return index_list\n",
    "\n",
    "    def generate_neg(self):\n",
    "        \"\"\"\n",
    "        generate negative sampling\n",
    "        :return: same shape as positive sampling\n",
    "        \"\"\"\n",
    "        neg_candidates, i = [], 0\n",
    "        neg_data = []\n",
    "        population = list(range(self.entity_num))\n",
    "        for idx, triple in enumerate(self.pos_data):\n",
    "            while True:\n",
    "                if i == len(neg_candidates):\n",
    "                    i = 0\n",
    "                    neg_candidates = random.choices(population=population, k=int(1e4))\n",
    "                neg, i = neg_candidates[i], i + 1\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    # replace head\n",
    "                    if neg not in self.related_dic[triple[2]]:\n",
    "                        neg_data.append([neg, triple[1], triple[2]])\n",
    "                        break\n",
    "                else:\n",
    "                    # replace tail\n",
    "                    if neg not in self.related_dic[triple[0]]:\n",
    "                        neg_data.append([triple[0], triple[1], neg])\n",
    "                        break\n",
    "\n",
    "        return np.array(neg_data)\n",
    "\n",
    "    def get_related_entity(self):\n",
    "        \"\"\"\n",
    "        get related entities\n",
    "        :return: {entity_id: {related_entity_id_1, related_entity_id_2...}}\n",
    "        \"\"\"\n",
    "        related_dic = dict()\n",
    "        for triple in self.pos_data:\n",
    "            if related_dic.get(triple[0]) is None:\n",
    "                related_dic[triple[0]] = {triple[2]}\n",
    "            else:\n",
    "                related_dic[triple[0]].add(triple[2])\n",
    "            if related_dic.get(triple[2]) is None:\n",
    "                related_dic[triple[2]] = {triple[0]}\n",
    "            else:\n",
    "                related_dic[triple[2]].add(triple[0])\n",
    "        return related_dic\n",
    "\n",
    "\n",
    "class TestSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super(TestSet, self).__init__()\n",
    "        self.raw_data = self.load_text()\n",
    "        self.data = self.raw_data\n",
    "        print(f\"Test set: {self.raw_data.shape[0]} triplets\")\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def load_text(self):\n",
    "        raw_data = pd.read_csv('/kg/transe/fb15k/freebase_mtr100_mte100-test.txt', sep='\\t', header=None,\n",
    "                               names=['head', 'relation', 'tail'],\n",
    "                               keep_default_na=False, encoding='utf-8')\n",
    "        raw_data = raw_data.applymap(lambda x: x.strip())\n",
    "        return raw_data.values\n",
    "\n",
    "    def convert_word_to_index(self, entity_to_index, relation_to_index, data):\n",
    "        index_list = np.array(\n",
    "            [[entity_to_index[triple[0]], relation_to_index[triple[1]], entity_to_index[triple[2]]] for triple in data])\n",
    "        self.data = index_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-16T09:22:49.310492Z",
     "end_time": "2024-03-16T09:22:49.320262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class TransR(nn.Module):\n",
    "    def __init__(self, ent_num, rel_num, device, dim=50, d_norn=2, margin=1):\n",
    "        '''\n",
    "        :param ent_num: entity_num\n",
    "        :param rel_num: relation_num\n",
    "        :param device: cuda_device\n",
    "        :param dim: dim = 50\n",
    "        :param d_norn: d_norm = 2\n",
    "        :param margin: margin hyperparameter\n",
    "        '''\n",
    "\n",
    "        super(TransR, self).__init__()\n",
    "        self.ent_num = ent_num\n",
    "        self.rel_num = rel_num\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.dim = dim\n",
    "        self.d_norn = d_norn\n",
    "        self.margin = margin\n",
    "\n",
    "        self.ent_emb = nn.Embedding.from_pretrained(\n",
    "            torch.empty(ent_num, self.dim).\n",
    "            uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim))\n",
    "            , freeze=False\n",
    "        )\n",
    "\n",
    "        self.rel_emb = nn.Embedding.from_pretrained(\n",
    "            torch.empty(rel_num, self.dim).\n",
    "            uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim))\n",
    "            , freeze=False\n",
    "        )\n",
    "\n",
    "        self.rel_proj_emb = nn.Embedding.from_pretrained(\n",
    "            torch.empty(rel_num, self.dim).\n",
    "            uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim))\n",
    "        )\n",
    "\n",
    "        rel_norm = torch.norm(self.rel_emb.weight.data, p=2, dim=1,keepdim=True)\n",
    "        self.rel_emb.weight.data = self.rel_emb.weight.data / rel_norm\n",
    "\n",
    "        rel_proj_norm= torch.norm(self.rel_proj_emb.weight.data, p=2, dim=1,keepdim=True)\n",
    "        self.rel_proj_emb.weight.data = self.rel_proj_emb.weight.data / rel_proj_norm\n",
    "\n",
    "    def forward(self, pos_head, pos_rel, pos_tail, neg_head, neg_rel, neg_tail):\n",
    "        \"\"\"\n",
    "        :param pos_head: [batch_size]\n",
    "        :param pos_relation: [batch_size]\n",
    "        :param pos_tail: [batch_size]\n",
    "        :param neg_head: [batch_size]\n",
    "        :param neg_relation: [batch_size]\n",
    "        :param neg_tail: [batch_size]\n",
    "        :return: triples loss\n",
    "        \"\"\"\n",
    "\n",
    "        pos_head_proj = self.ent_emb(pos_head)@self.rel_proj_emb(pos_rel)\n",
    "        pos_tail_proj = self.ent_emb(pos_tail)@self.rel_proj_emb(pos_rel)\n",
    "\n",
    "        neg_head_proj = self.ent_emb(neg_head)@self.rel_proj_emb(neg_rel)\n",
    "        neg_tail_proj = self.ent_emb(neg_tail)@self.rel_proj_emb(neg_rel)\n",
    "\n",
    "        pos_dis = pos_head_proj + self.rel_emb(pos_rel) - pos_tail_proj\n",
    "        neg_dis = neg_head_proj + self.rel_emb(neg_rel) - neg_tail_proj\n",
    "\n",
    "        return self.calculate_loss(pos_dis, neg_dis)\n",
    "\n",
    "    def calculate_loss(self, pos_dis, neg_dis):\n",
    "        '''\n",
    "        :param pos_dis: [batch_size, embed_dim]\n",
    "        :param neg_dis: [batch_size, embed_dim]\n",
    "        :return: triples loss: [batch_size]\n",
    "        '''\n",
    "\n",
    "        distance_diff = self.margin + torch.norm(pos_dis, p=self.d_norn, dim=-1) -\\\n",
    "                        torch.norm(neg_dis, p=self.d_norn, dim=-1)\n",
    "\n",
    "        return torch.sum(F.relu(distance_diff))\n",
    "\n",
    "    def tail_predict(self, head, relation, tail, k=10):\n",
    "        '''\n",
    "        to do tail prediction hits@k\n",
    "        :param head: [batch_size]\n",
    "        :param relation: [batch_size]\n",
    "        :param tail: [batch_size]\n",
    "        :param k: hits@k\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # head: [batch_size]\n",
    "\n",
    "        print(self.ent_emb(head).shape, self.rel_emb(relation).shape)\n",
    "        # h_r\n",
    "        head_proj_and_r = torch.sum(self.ent_emb(head) * self.rel_proj_emb(relation), dim=1, keepdim=True)\n",
    "\n",
    "        head_proj_and_r = torch.unsqueeze(head_proj_and_r, dim=1)\n",
    "\n",
    "        # embed_tail = self.ent_emb.weight.data.expand(head_proj_and_r.shape[0], self.ent_num, self.dim)\n",
    "\n",
    "        tail_r = torch.sum(self.ent_emb(tail) * self.rel_proj_emb(relation), dim=1, keepdim=True)\n",
    "        tail_r = torch.unsqueeze(tail_r, dim=1)\n",
    "\n",
    "        print(tail_r.shape)\n",
    "\n",
    "        values, indices = torch.topk(torch.norm(head_proj_and_r - tail_r, dim=2), 1, dim=1, largest=False)\n",
    "\n",
    "        tail = tail.view(-1, 1)\n",
    "        return torch.sum(torch.eq(indices, tail)).item()\n",
    "\n",
    "\n",
    "# 创建一个 transH 模型的实例\n",
    "ent_num = 1000  # 实体数\n",
    "rel_num = 100   # 关系数\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = TransR(ent_num, rel_num, device)\n",
    "\n",
    "head = torch.tensor([0, 1, 2])  # 头实体\n",
    "relation = torch.tensor([0, 1, 2])  # 关系\n",
    "tail = torch.tensor([1, 2, 3])  # 真实尾实体"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-16T09:22:49.328218Z",
     "end_time": "2024-03-16T09:22:49.379260Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 14951 entities, 1345 relations, 483142 triplets.\n",
      "Test set: 59071 triplets\n",
      "epoch 1, loss = 0.8019502138822153\n",
      "===>epoch 1, test accuracy 0.03128438658563424\n",
      "epoch 2, loss = 0.5702923585811023\n",
      "===>epoch 2, test accuracy 0.03944405884444144\n",
      "epoch 3, loss = 0.41035760996049775\n",
      "===>epoch 3, test accuracy 0.04584313791877571\n",
      "epoch 4, loss = 0.3081991608253066\n",
      "===>epoch 4, test accuracy 0.04496284132653925\n",
      "epoch 5, loss = 0.25018822323555445\n",
      "===>epoch 5, test accuracy 0.04816238086370639\n",
      "epoch 6, loss = 0.21585200718738728\n",
      "===>epoch 6, test accuracy 0.050227692099338084\n",
      "epoch 7, loss = 0.19295219421992707\n",
      "===>epoch 7, test accuracy 0.05075248429855597\n",
      "epoch 8, loss = 0.17737669248489463\n",
      "===>epoch 8, test accuracy 0.05324101504968597\n",
      "epoch 9, loss = 0.16559673930725893\n",
      "===>epoch 9, test accuracy 0.05357959066208461\n",
      "epoch 10, loss = 0.15636402568119226\n",
      "===>epoch 10, test accuracy 0.05567875945895617\n",
      "epoch 11, loss = 0.1489061511595839\n",
      "===>epoch 11, test accuracy 0.059115301924802355\n",
      "epoch 12, loss = 0.1423241923677418\n",
      "===>epoch 12, test accuracy 0.058861370215503375\n",
      "epoch 13, loss = 0.1370682602851835\n",
      "===>epoch 13, test accuracy 0.05821807655194596\n",
      "epoch 14, loss = 0.13292367678280453\n",
      "===>epoch 14, test accuracy 0.06009717120075841\n",
      "epoch 15, loss = 0.1295420118314229\n",
      "===>epoch 15, test accuracy 0.059436948756581065\n",
      "epoch 16, loss = 0.12606950999604113\n",
      "===>epoch 16, test accuracy 0.0610959692573344\n",
      "epoch 17, loss = 0.12251796136375806\n",
      "===>epoch 17, test accuracy 0.05925073216976181\n",
      "epoch 18, loss = 0.12004727532568607\n",
      "===>epoch 18, test accuracy 0.059352304853481405\n",
      "epoch 19, loss = 0.11776017077534047\n",
      "===>epoch 19, test accuracy 0.059183017047282085\n",
      "epoch 20, loss = 0.11611423404701625\n",
      "===>epoch 20, test accuracy 0.0589798716798429\n",
      "epoch 21, loss = 0.11406604871940616\n",
      "===>epoch 21, test accuracy 0.06036803169067732\n",
      "epoch 22, loss = 0.11226582552870701\n",
      "===>epoch 22, test accuracy 0.06041881803253712\n",
      "epoch 23, loss = 0.11033794332313807\n",
      "===>epoch 23, test accuracy 0.06080817998679555\n",
      "epoch 24, loss = 0.10933681585225563\n",
      "===>epoch 24, test accuracy 0.0601479575426182\n",
      "epoch 25, loss = 0.10817274141057308\n",
      "===>epoch 25, test accuracy 0.060029456078278684\n",
      "epoch 26, loss = 0.10667465088102673\n",
      "===>epoch 26, test accuracy 0.06074046486431582\n",
      "epoch 27, loss = 0.10565620713435661\n",
      "===>epoch 27, test accuracy 0.06055424827749657\n",
      "epoch 28, loss = 0.10459087000491538\n",
      "===>epoch 28, test accuracy 0.06063889218059623\n",
      "epoch 29, loss = 0.10355424392453368\n",
      "===>epoch 29, test accuracy 0.0606727497418361\n",
      "epoch 30, loss = 0.10241553892213218\n",
      "===>epoch 30, test accuracy 0.059623165343400314\n",
      "epoch 31, loss = 0.10157797577391783\n",
      "===>epoch 31, test accuracy 0.060114099981378344\n",
      "epoch 32, loss = 0.10072275572905193\n",
      "===>epoch 32, test accuracy 0.06009717120075841\n",
      "epoch 33, loss = 0.09990725666225961\n",
      "===>epoch 33, test accuracy 0.05967395168526011\n",
      "epoch 34, loss = 0.09958433364657336\n",
      "===>epoch 34, test accuracy 0.05947080631782093\n",
      "epoch 35, loss = 0.09894448055318186\n",
      "===>epoch 35, test accuracy 0.06055424827749657\n",
      "epoch 36, loss = 0.0977332603624436\n",
      "===>epoch 36, test accuracy 0.058590509725584466\n",
      "epoch 37, loss = 0.09704598872618392\n",
      "===>epoch 37, test accuracy 0.06018181510385807\n",
      "epoch 38, loss = 0.0966123220206218\n",
      "===>epoch 38, test accuracy 0.06045267559377698\n",
      "epoch 39, loss = 0.09637767020828311\n",
      "===>epoch 39, test accuracy 0.06060503461935637\n",
      "epoch 40, loss = 0.09575868693151054\n",
      "===>epoch 40, test accuracy 0.06101132535423474\n",
      "epoch 41, loss = 0.09507490362555944\n",
      "===>epoch 41, test accuracy 0.06013102876199827\n",
      "epoch 42, loss = 0.0945159746452969\n",
      "===>epoch 42, test accuracy 0.05999559851703882\n",
      "epoch 43, loss = 0.09421045005161864\n",
      "===>epoch 43, test accuracy 0.06046960437439691\n",
      "epoch 44, loss = 0.09400601613820178\n",
      "===>epoch 44, test accuracy 0.0606727497418361\n",
      "epoch 45, loss = 0.09359917682295861\n",
      "===>epoch 45, test accuracy 0.061214470721673915\n",
      "epoch 46, loss = 0.09301858834811622\n",
      "===>epoch 46, test accuracy 0.06052039071625671\n",
      "epoch 47, loss = 0.09263235207336241\n",
      "===>epoch 47, test accuracy 0.06131604340539351\n",
      "epoch 48, loss = 0.09216043958500758\n",
      "===>epoch 48, test accuracy 0.06006331363951854\n",
      "epoch 49, loss = 0.09153070217367755\n",
      "===>epoch 49, test accuracy 0.061519188772832696\n",
      "epoch 50, loss = 0.09161747214710475\n",
      "===>epoch 50, test accuracy 0.060283387787577664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device('cuda')\n",
    "embed_dim = 50\n",
    "num_epochs = 50\n",
    "train_batch_size = 32\n",
    "test_batch_size = 256\n",
    "lr = 1e-2\n",
    "momentum = 0\n",
    "gamma = 1\n",
    "d_norm = 2\n",
    "top_k = 10\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_dataset = TrainSet()\n",
    "    test_dataset = TestSet()\n",
    "    test_dataset.convert_word_to_index(train_dataset.entity_to_index, train_dataset.relation_to_index,\n",
    "                                       test_dataset.raw_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "    transh = TransR(train_dataset.entity_num, train_dataset.relation_num, device, dim=embed_dim, d_norn=d_norm, margin=gamma).to(device)\n",
    "    optimizer = optim.SGD(transh.parameters(), lr=lr, momentum=momentum)\n",
    "    for epoch in range(num_epochs):\n",
    "        # e <= e / ||e||\n",
    "        entity_norm = torch.norm(transh.ent_emb.weight.data, dim=1, keepdim=True)\n",
    "        transh.ent_emb.weight.data = transh.ent_emb.weight.data / entity_norm\n",
    "        total_loss = 0\n",
    "        for batch_idx, (pos, neg) in enumerate(train_loader):\n",
    "            pos, neg = pos.to(device), neg.to(device)\n",
    "            # pos: [batch_size, 3] => [3, batch_size]\n",
    "            pos = torch.transpose(pos, 0, 1)\n",
    "            # pos_head, pos_relation, pos_tail: [batch_size]\n",
    "            pos_head, pos_relation, pos_tail = pos[0], pos[1], pos[2]\n",
    "            neg = torch.transpose(neg, 0, 1)\n",
    "            # neg_head, neg_relation, neg_tail: [batch_size]\n",
    "            neg_head, neg_relation, neg_tail = neg[0], neg[1], neg[2]\n",
    "            loss = transh(pos_head, pos_relation, pos_tail, neg_head, neg_relation, neg_tail)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch {epoch+1}, loss = {total_loss/train_dataset.__len__()}\")\n",
    "        corrct_test = 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            # data: [batch_size, 3] => [3, batch_size]\n",
    "            data = torch.transpose(data, 0, 1)\n",
    "            corrct_test += transh.tail_predict(data[0], data[1], data[2], k=top_k)\n",
    "        print(f\"===>epoch {epoch+1}, test accuracy {corrct_test/test_dataset.__len__()}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-16T09:22:49.378516Z",
     "end_time": "2024-03-16T10:05:19.632703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-16T10:05:19.635389Z",
     "end_time": "2024-03-16T10:05:19.680055Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
