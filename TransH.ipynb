{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-03-15T12:43:11.210354Z",
     "end_time": "2024-03-15T12:43:14.142453Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "class TrainSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super(TrainSet, self).__init__()\n",
    "        # self.raw_data, self.entity_dic, self.relation_dic = self.load_texd()\n",
    "        self.raw_data, self.entity_to_index, self.relation_to_index = self.load_text()\n",
    "        self.entity_num, self.relation_num = len(self.entity_to_index), len(self.relation_to_index)\n",
    "        self.triple_num = self.raw_data.shape[0]\n",
    "        print(f'Train set: {self.entity_num} entities, {self.relation_num} relations, {self.triple_num} triplets.')\n",
    "        self.pos_data = self.convert_word_to_index(self.raw_data)\n",
    "        self.related_dic = self.get_related_entity()\n",
    "        # print(self.related_dic[0], self.related_dic[479])\n",
    "        self.neg_data = self.generate_neg()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.triple_num\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return [self.pos_data[item], self.neg_data[item]]\n",
    "\n",
    "    def load_text(self):\n",
    "        raw_data = pd.read_csv('/kg/transe/fb15k/freebase_mtr100_mte100-train.txt', sep='\\t', header=None,\n",
    "                               names=['head', 'relation', 'tail'],\n",
    "                               keep_default_na=False, encoding='utf-8')\n",
    "        raw_data = raw_data.applymap(lambda x: x.strip())\n",
    "        head_count = Counter(raw_data['head'])\n",
    "        tail_count = Counter(raw_data['tail'])\n",
    "        relation_count = Counter(raw_data['relation'])\n",
    "        entity_list = list((head_count + tail_count).keys())\n",
    "        relation_list = list(relation_count.keys())\n",
    "        entity_dic = dict([(word, idx) for idx, word in enumerate(entity_list)])\n",
    "        relation_dic = dict([(word, idx) for idx, word in enumerate(relation_list)])\n",
    "        return raw_data.values, entity_dic, relation_dic\n",
    "\n",
    "    def convert_word_to_index(self, data):\n",
    "        index_list = np.array([\n",
    "            [self.entity_to_index[triple[0]], self.relation_to_index[triple[1]], self.entity_to_index[triple[2]]] for\n",
    "            triple in data])\n",
    "        return index_list\n",
    "\n",
    "    def generate_neg(self):\n",
    "        \"\"\"\n",
    "        generate negative sampling\n",
    "        :return: same shape as positive sampling\n",
    "        \"\"\"\n",
    "        neg_candidates, i = [], 0\n",
    "        neg_data = []\n",
    "        population = list(range(self.entity_num))\n",
    "        for idx, triple in enumerate(self.pos_data):\n",
    "            while True:\n",
    "                if i == len(neg_candidates):\n",
    "                    i = 0\n",
    "                    neg_candidates = random.choices(population=population, k=int(1e4))\n",
    "                neg, i = neg_candidates[i], i + 1\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    # replace head\n",
    "                    if neg not in self.related_dic[triple[2]]:\n",
    "                        neg_data.append([neg, triple[1], triple[2]])\n",
    "                        break\n",
    "                else:\n",
    "                    # replace tail\n",
    "                    if neg not in self.related_dic[triple[0]]:\n",
    "                        neg_data.append([triple[0], triple[1], neg])\n",
    "                        break\n",
    "\n",
    "        return np.array(neg_data)\n",
    "\n",
    "    def get_related_entity(self):\n",
    "        \"\"\"\n",
    "        get related entities\n",
    "        :return: {entity_id: {related_entity_id_1, related_entity_id_2...}}\n",
    "        \"\"\"\n",
    "        related_dic = dict()\n",
    "        for triple in self.pos_data:\n",
    "            if related_dic.get(triple[0]) is None:\n",
    "                related_dic[triple[0]] = {triple[2]}\n",
    "            else:\n",
    "                related_dic[triple[0]].add(triple[2])\n",
    "            if related_dic.get(triple[2]) is None:\n",
    "                related_dic[triple[2]] = {triple[0]}\n",
    "            else:\n",
    "                related_dic[triple[2]].add(triple[0])\n",
    "        return related_dic\n",
    "\n",
    "\n",
    "class TestSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super(TestSet, self).__init__()\n",
    "        self.raw_data = self.load_text()\n",
    "        self.data = self.raw_data\n",
    "        print(f\"Test set: {self.raw_data.shape[0]} triplets\")\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def load_text(self):\n",
    "        raw_data = pd.read_csv('/kg/transe/fb15k/freebase_mtr100_mte100-test.txt', sep='\\t', header=None,\n",
    "                               names=['head', 'relation', 'tail'],\n",
    "                               keep_default_na=False, encoding='utf-8')\n",
    "        raw_data = raw_data.applymap(lambda x: x.strip())\n",
    "        return raw_data.values\n",
    "\n",
    "    def convert_word_to_index(self, entity_to_index, relation_to_index, data):\n",
    "        index_list = np.array(\n",
    "            [[entity_to_index[triple[0]], relation_to_index[triple[1]], entity_to_index[triple[2]]] for triple in data])\n",
    "        self.data = index_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T12:43:14.158432Z",
     "end_time": "2024-03-15T12:43:14.593156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TransH(nn.Module):\n",
    "    def __init__(self, ent_num, rel_num, device, dim=50, d_norn=2, margin=1):\n",
    "        '''\n",
    "        :param ent_num: entity_num\n",
    "        :param rel_num: relation_num\n",
    "        :param device: cuda_device\n",
    "        :param dim: dim = 50\n",
    "        :param d_norn: d_norm = 2\n",
    "        :param margin: margin hyperparameter\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ent_num = ent_num\n",
    "        self.rel_num = rel_num\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.dim = dim\n",
    "        self.d_norn = d_norn\n",
    "        self.margin = torch.tensor([margin]).to(self.device)\n",
    "\n",
    "        self.ent_emb = nn.Embedding.from_pretrained(\n",
    "            torch.empty(ent_num, self.dim).uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim))\n",
    "            , freeze=False\n",
    "        )\n",
    "\n",
    "        self.rel_emb = nn.Embedding.from_pretrained(\n",
    "            torch.empty(rel_num, self.dim).uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim))\n",
    "            , freeze=False\n",
    "        )\n",
    "\n",
    "        self.rel_hyper_emb = nn.Embedding.from_pretrained(\n",
    "            torch.empty(rel_num, self.dim).uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim))\n",
    "            , freeze=False\n",
    "        )\n",
    "\n",
    "        rel_norm = torch.norm(self.rel_emb.weight.data, dim=1, keepdim=True)\n",
    "        self.rel_emb.weight.data = self.rel_emb.weight.data / rel_norm\n",
    "\n",
    "        rel_hyper_norm  =  torch.norm(self.rel_hyper_emb.weight.data, dim=1, keepdim=True)\n",
    "        self.rel_hyper_emb.weight.data = self.rel_hyper_emb.weight.data / rel_hyper_norm\n",
    "\n",
    "    def forward(self, pos_head, pos_rel, pos_tail, neg_head, neg_rel, neg_tail):\n",
    "        '''\n",
    "        :param pos_head: [batch_size]\n",
    "        :param pos_relation: [batch_size]\n",
    "        :param pos_tail: [batch_size]\n",
    "        :param neg_head: [batch_size]\n",
    "        :param neg_relation: [batch_size]\n",
    "        :param neg_tail: [batch_size]\n",
    "        :return: triple_loss\n",
    "        '''\n",
    "\n",
    "        pos_head_proj = self.ent_emb(pos_head) - self.rel_hyper_emb(pos_rel) * \\\n",
    "        torch.sum(self.ent_emb(pos_head) * self.rel_hyper_emb(pos_rel), dim=1, keepdim=True)\n",
    "\n",
    "        pos_tail_proj = self.ent_emb(pos_tail) - self.rel_hyper_emb(pos_rel) * \\\n",
    "        torch.sum(self.ent_emb(pos_tail) * self.rel_hyper_emb(pos_rel), dim=1, keepdim=True)\n",
    "\n",
    "        neg_head_proj = self.ent_emb(neg_head) - self.rel_hyper_emb(neg_rel) * \\\n",
    "        torch.sum(self.ent_emb(pos_head) * self.rel_hyper_emb(pos_rel), dim=1, keepdim=True)\n",
    "\n",
    "        neg_tail_proj = self.ent_emb(neg_tail) - self.rel_hyper_emb(neg_rel) * \\\n",
    "        torch.sum(self.ent_emb(pos_tail) * self.rel_hyper_emb(pos_rel), dim=1, keepdim=True)\n",
    "\n",
    "        pos_dis = pos_head_proj + self.rel_emb(pos_rel) - pos_tail_proj\n",
    "\n",
    "        neg_dis = neg_head_proj + self.rel_emb(neg_rel) - neg_tail_proj\n",
    "\n",
    "        self.calculate_loss(pos_dis, neg_dis)\n",
    "        return self.calculate_loss(pos_dis, neg_dis).requires_grad_()\n",
    "\n",
    "    def calculate_loss(self, pos_dis, neg_dis):\n",
    "        '''\n",
    "        :param pos_dis: [batch_size, embed_dim]\n",
    "        :param neg_dis: [batch_size, embed_dim]\n",
    "        :return: triples loss: [batch_size]\n",
    "        '''\n",
    "\n",
    "        distance_diff = self.margin + torch.norm(pos_dis, p=self.d_norn, dim=-1) - \\\n",
    "            torch.norm(neg_dis, p=self.d_norn, dim=-1)\n",
    "\n",
    "        return torch.sum(F.relu(distance_diff))\n",
    "\n",
    "    def tail_predict(self, head, relation, tail, k=10):\n",
    "        '''\n",
    "        to do tail prediction hits@k\n",
    "        :param head: [batch_size]\n",
    "        :param relation: [batch_size]\n",
    "        :param tail: [batch_size]\n",
    "        :param k: hits@k\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # head: [batch_size]\n",
    "        # head_proj: [batch_size, embed_size]\n",
    "        # head_proj_and_r: [batch_size, embed_size] => [batch_size, 1, embed_size] => [batch_size, N, embed_size]\n",
    "\n",
    "        head_proj = self.ent_emb(head) - self.rel_hyper_emb(relation) * \\\n",
    "                        torch.sum(self.ent_emb(head) * self.rel_hyper_emb(relation), dim=1, keepdim=True)\n",
    "\n",
    "        head_proj_and_r = self.rel_emb(relation) + head_proj\n",
    "        head_proj_and_r = torch.unsqueeze(head_proj_and_r, dim=1)\n",
    "        head_proj_and_r = head_proj_and_r.expand(head_proj_and_r.shape[0], self.ent_num, self.dim)\n",
    "\n",
    "        # embed_tail: [batch_size, N, embed_size]\n",
    "        embed_tail = self.ent_emb.weight.data.expand(head_proj_and_r.shape[0], self.ent_num, self.dim)\n",
    "\n",
    "        # indices: [batch_size, k]\n",
    "        values, indices = torch.topk(torch.norm(head_proj_and_r - embed_tail, dim=2),\n",
    "                                     k, dim=1, largest=False)\n",
    "\n",
    "        # tail: [batch_size] => [batch_size, 1]\n",
    "        tail = tail.view(-1, 1)\n",
    "        return torch.sum(torch.eq(indices, tail)).item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T12:43:14.600155Z",
     "end_time": "2024-03-15T12:43:14.617861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 14951 entities, 1345 relations, 483142 triplets.\n",
      "Test set: 59071 triplets\n",
      "epoch 1, loss = 0.7811864041838371\n",
      "===>epoch 1, test accuracy 0.19530734201215486\n",
      "epoch 2, loss = 0.5942808710526185\n",
      "===>epoch 2, test accuracy 0.21013695383521525\n",
      "epoch 3, loss = 0.4655395803484068\n",
      "===>epoch 3, test accuracy 0.22596536371485162\n",
      "epoch 4, loss = 0.35766450585006365\n",
      "===>epoch 4, test accuracy 0.24384215604949974\n",
      "epoch 5, loss = 0.27931710828004025\n",
      "===>epoch 5, test accuracy 0.25762218347412436\n",
      "epoch 6, loss = 0.22691859197629605\n",
      "===>epoch 6, test accuracy 0.267170015743766\n",
      "epoch 7, loss = 0.19198953094764729\n",
      "===>epoch 7, test accuracy 0.273128946521982\n",
      "epoch 8, loss = 0.16732999158072076\n",
      "===>epoch 8, test accuracy 0.2791725212032977\n",
      "epoch 9, loss = 0.14917287186066536\n",
      "===>epoch 9, test accuracy 0.2833031436745611\n",
      "epoch 10, loss = 0.13513329961975848\n",
      "===>epoch 10, test accuracy 0.2876369115132637\n",
      "epoch 11, loss = 0.12388734922595354\n",
      "===>epoch 11, test accuracy 0.28804320224814206\n",
      "epoch 12, loss = 0.11457421881340267\n",
      "===>epoch 12, test accuracy 0.29125967056592916\n",
      "epoch 13, loss = 0.10677429801997186\n",
      "===>epoch 13, test accuracy 0.29093802373415045\n",
      "epoch 14, loss = 0.10012518237216367\n",
      "===>epoch 14, test accuracy 0.28909278664657784\n",
      "epoch 15, loss = 0.09449291519263403\n",
      "===>epoch 15, test accuracy 0.2917167476426673\n",
      "epoch 16, loss = 0.08963285973791628\n",
      "===>epoch 16, test accuracy 0.28954986372331604\n",
      "epoch 17, loss = 0.08542594174017397\n",
      "===>epoch 17, test accuracy 0.2892620744527772\n",
      "epoch 18, loss = 0.08177064021879298\n",
      "===>epoch 18, test accuracy 0.28966836518765554\n",
      "epoch 19, loss = 0.07856505449017245\n",
      "===>epoch 19, test accuracy 0.2877384841969833\n",
      "epoch 20, loss = 0.07575669428513257\n",
      "===>epoch 20, test accuracy 0.2879416295644225\n",
      "epoch 21, loss = 0.07322645165813065\n",
      "===>epoch 21, test accuracy 0.2875861251714039\n",
      "epoch 22, loss = 0.07097080420378098\n",
      "===>epoch 22, test accuracy 0.2865196119923482\n",
      "epoch 23, loss = 0.06892720608055888\n",
      "===>epoch 23, test accuracy 0.28702747541094614\n",
      "epoch 24, loss = 0.06706786319003953\n",
      "===>epoch 24, test accuracy 0.28489444905283473\n",
      "epoch 25, loss = 0.06537308763045081\n",
      "===>epoch 25, test accuracy 0.28728140712024514\n",
      "epoch 26, loss = 0.06383190223286782\n",
      "===>epoch 26, test accuracy 0.28553774271639215\n",
      "epoch 27, loss = 0.062397147510132964\n",
      "===>epoch 27, test accuracy 0.28765384029388363\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device('cpu')\n",
    "embed_dim = 50\n",
    "num_epochs = 50\n",
    "train_batch_size = 32\n",
    "test_batch_size = 256\n",
    "lr = 1e-2\n",
    "momentum = 0\n",
    "gamma = 1\n",
    "d_norm = 2\n",
    "top_k = 10\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_dataset = TrainSet()\n",
    "    test_dataset = TestSet()\n",
    "    test_dataset.convert_word_to_index(train_dataset.entity_to_index, train_dataset.relation_to_index,\n",
    "                                       test_dataset.raw_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "    transh = TransH(train_dataset.entity_num, train_dataset.relation_num, device, dim=embed_dim, d_norn=d_norm, margin=gamma).to(device)\n",
    "    optimizer = optim.SGD(transh.parameters(), lr=lr, momentum=momentum)\n",
    "    for epoch in range(num_epochs):\n",
    "        # e <= e / ||e||\n",
    "        entity_norm = torch.norm(transh.ent_emb.weight.data, dim=1, keepdim=True)\n",
    "        transh.ent_emb.weight.data = transh.ent_emb.weight.data / entity_norm\n",
    "        total_loss = 0\n",
    "        for batch_idx, (pos, neg) in enumerate(train_loader):\n",
    "            pos, neg = pos.to(device), neg.to(device)\n",
    "            # pos: [batch_size, 3] => [3, batch_size]\n",
    "            pos = torch.transpose(pos, 0, 1)\n",
    "            # pos_head, pos_relation, pos_tail: [batch_size]\n",
    "            pos_head, pos_relation, pos_tail = pos[0], pos[1], pos[2]\n",
    "            neg = torch.transpose(neg, 0, 1)\n",
    "            # neg_head, neg_relation, neg_tail: [batch_size]\n",
    "            neg_head, neg_relation, neg_tail = neg[0], neg[1], neg[2]\n",
    "            loss = transh(pos_head, pos_relation, pos_tail, neg_head, neg_relation, neg_tail)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch {epoch+1}, loss = {total_loss/train_dataset.__len__()}\")\n",
    "        corrct_test = 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            # data: [batch_size, 3] => [3, batch_size]\n",
    "            data = torch.transpose(data, 0, 1)\n",
    "            corrct_test += transh.tail_predict(data[0], data[1], data[2], k=top_k)\n",
    "        print(f\"===>epoch {epoch+1}, test accuracy {corrct_test/test_dataset.__len__()}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
